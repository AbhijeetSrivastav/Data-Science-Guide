{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas library is specifically designed for data analysis and manipulation. \n",
    "\n",
    "It offers a variety of features and functionalities that make it an essential tool for anyone working with data in Python. \n",
    "\n",
    "\n",
    "**Pandas offers:**\n",
    "\n",
    "* **Data Structures:** Pandas provides two main data structures: Series (one-dimensional labeled array) and DataFrame (two-dimensional labeled data structure similar to a spreadsheet). These structures allow us to store and organize our data efficiently.\n",
    "* **Data Cleaning and Manipulation:** Pandas offers extensive tools for cleaning and manipulating our data, including handling missing values, filtering, sorting, and aggregating data.\n",
    "* **Data Analysis:** Pandas comes with a rich set of statistical and analytical functions, allowing us to perform various analyses on your data, such as calculating descriptive statistics, finding correlations, and creating visualizations.\n",
    "* **Time Series Analysis:** Pandas has specialized functionalities for working with time series data, making it a valuable tool for financial analysis, weather forecasting, and other time-dependent applications.\n",
    "* **Integration with Other Libraries:** Pandas seamlessly integrates with other popular Python libraries like NumPy, Matplotlib, and Scikit-learn, allowing you to build powerful data analysis workflows.\n",
    "\n",
    "\n",
    "**Advantages of pandas:**\n",
    "\n",
    "* **Powerful and Efficient:** Pandas is designed for speed and efficiency, making it suitable for working with large datasets.\n",
    "* **Easy to Learn:** The syntax of pandas is relatively simple and intuitive, even for beginners in Python.\n",
    "* **Versatile:** Pandas can be used for a wide range of data analysis tasks, from basic cleaning and manipulation to complex statistical analysis and visualization.\n",
    "* **Popular and Well-Supported:** Pandas has a large and active community, meaning we can easily find resources, tutorials, and help online.\n",
    "* **Optimized Data Structures:** Pandas leverages optimized data structures like NumPy arrays and Series internally, providing efficient memory management and fast access to data elements.\n",
    "* **Vectorized Operations:** Pandas uses vectorized operations instead of looping, allowing it to perform calculations on entire arrays of data simultaneously, significantly speeding up computations compared to traditional Python loops.\n",
    "* **C-Level Optimizations:** Pandas utilizes C-level code for critical operations, further enhancing performance and memory efficiency compared to pure Python implementations.\n",
    "* **Lazy Evaluation:** Pandas employs lazy evaluation, delaying expensive computations until they are absolutely necessary. This improves performance for interactive analysis and exploration.\n",
    "* **Wide Range of Data Types:** Pandas supports various data types, including numerics, strings, categorical, datetimes, and more, allowing for flexible data manipulation and analysis.\n",
    "\n",
    "\n",
    "**Disadvantages of Pandas:**\n",
    "\n",
    "* **Not Ideal for Big Data:** Although pandas can handle large datasets, it might not be the best choice for truly massive datasets typically associated with \"big data\" applications. For such scenarios, libraries like Spark are designed to handle distributed processing and scale efficiently.\n",
    "* **Limited Support for Unstructured Data:** Pandas primarily focuses on structured data in tabular formats. While it can handle some unstructured data manipulation, it's not ideal for complex processing of text, images, or other non-tabular data types. Libraries like spaCy or OpenCV are better suited for such tasks.\n",
    "* **Learning Curve for Advanced Features:** While the core functionalities of pandas are relatively easy to learn, mastering its advanced features like `groupby` operations, custom functions, and complex data transformations can have a steeper learning curve.\n",
    "* **Memory Overhead:** Data structures in pandas come with some memory overhead compared to raw NumPy arrays, which can be an issue for extremely large datasets. Pandas can be memory-intensive, especially when working with large datasets. Its data structures are designed for flexibility and ease of use, but they might not be the most memory-efficient choice for massive datasets.\n",
    "* **Limited Parallelism:** While pandas allows some parallelization, it's not optimized for large-scale distributed computing like libraries like Dask or Spark. This can limit performance for massive datasets requiring parallel processing across multiple cores or machines.\n",
    "* **GIL (Global Interpreter Lock):** Python's GIL can limit performance for CPU-bound operations in pandas, especially on multi-core systems. However, recent versions offer experimental parallelism capabilities to mitigate this issue.\n",
    "  \n",
    "\n",
    "**Pandas vs Numpy**\n",
    "\n",
    "* **When to use pandas:**\n",
    "  * Structured data\n",
    "  * Data cleaning and manipulation\n",
    "  * Statistical analysis and exploration\n",
    "  * Time series analysis\n",
    "  * Data visualization\n",
    "* **When to use numpy:**\n",
    "  * Purely numerical operations\n",
    "  * Limited data size\n",
    "  * Specific data types\n",
    "  \n",
    "\n",
    "[Docs Reference](https://pandas.pydata.org/docs/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early Days (2008-2010):**\n",
    "\n",
    "* **Conception:** Wes McKinney, frustrated by the lack of efficient tools for data analysis in Python, began developing Pandas in 2008.\n",
    "* **Inspiration:** Drawing inspiration from R's DataFrames and NumPy's arrays, McKinney aimed to create a library that combined the strengths of both.\n",
    "* **Initial Release:** In 2010, the first public version of Pandas (0.1.0) was released. It included basic DataFrame and Series functionalities, focused primarily on financial analysis.\n",
    "\n",
    "\n",
    "**Growth and Adoption (2011-2015):**\n",
    "\n",
    "* **Rapid Development:** Pandas gained significant traction due to its intuitive interface, efficient data structures, and growing feature set.\n",
    "* **Community Contributions:** An active community of developers began contributing features, bug fixes, and documentation, accelerating Pandas' development.\n",
    "* **Integration with SciPy Stack:** Pandas became a core component of the SciPy stack, solidifying its position as a key tool for data analysis in Python.\n",
    "* **Integration with Numpy Stack:** Pandas became a core component of the Numpy stack, solidifying its position as a key tool for data analysis in Python in 2011.\n",
    "* **Integration with Matplotlib and Seaborn Stack:** Pandas became a core component of the Matplotlib and Seaborn stack, solidifying its position as a key tool for data analysis in Python in 2015.\n",
    "* **Introduction of Time Series features:** Time series functionalities implemented for financial and scientific analysis in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Architecture of Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Core Data Structures:**\n",
    "\n",
    "* **Series:** One-dimensional labeled array, like a column from a spreadsheet. It holds data of any type (numeric, string, etc.) and is indexed by labels.\n",
    "* **DataFrame:** Two-dimensional labeled data structure, like a spreadsheet. It consists of columns (Series) with different data types and is indexed by rows and columns.\n",
    "* **Panel (deprecated):** Three-dimensional analogous to DataFrames, but less commonly used and deprecated in recent versions.\n",
    "\n",
    "\n",
    "**B. Internal Building Blocks:**\n",
    "\n",
    "* **BlockManager:** The heart of Pandas, responsible for managing the physical memory layout of data in Series and DataFrames. It uses NumPy arrays internally for efficient storage and retrieval of data.\n",
    "* **Index:** Represents the labels for rows and columns in Series and DataFrames. It can be numeric, categorical, or custom objects.\n",
    "* **DataType:** Defines the type of data stored in a Series or DataFrame column (e.g., integer, string, datetime).\n",
    "\n",
    "\n",
    "**C. Key Architectural Aspects:**\n",
    "\n",
    "* **Vectorized Operations:** Pandas leverages vectorized operations, performing calculations on entire arrays at once instead of individual elements, leading to significant performance gains.\n",
    "* **Mutable vs. Immutable:** While Series and DataFrames are mutable (changeable), some internal data structures like NumPy arrays are immutable (unchangeable) for efficiency and data integrity.\n",
    "* **Lazy Evaluation:** Pandas employs lazy evaluation, delaying computations until necessary, improving performance for interactive analysis and exploration.\n",
    "* **Integration with NumPy:** Pandas builds upon NumPy arrays for efficient data storage and manipulation, offering a seamless experience for numerical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Objects in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Series:**\n",
    "\n",
    "* Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). \n",
    "* The axis labels are collectively referred to as the index.\n",
    "* Imagine it like a single column from a spreadsheet with labels for each element.\n",
    "* Used for storing and manipulating sequences of data.\n",
    "* Size of series is fixed and it has only one data type which is assigned at time of initialization or declaration.\n",
    "* Series supports vectorized operations.\n",
    "\n",
    "\n",
    "**B. DataFrame:**\n",
    "\n",
    "* DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. \n",
    "* We can think of it like a spreadsheet or SQL table, or a dict of Series objects.\n",
    "* The primary workhorse of Pandas, used for storing, manipulating, and analyzing tabular data.\n",
    "* DataFrame accepts many different kinds of input:\n",
    "  * Dict of 1D ndarrays, lists, dicts, or Series\n",
    "  * 2-D numpy.ndarray\n",
    "  * Structured or record ndarray\n",
    "  * A Series\n",
    "  * Another DataFrame\n",
    "\n",
    "\n",
    "**C. Index:**\n",
    "\n",
    "* Represents the labels for rows and columns in Series and DataFrames.\n",
    "* Can be numeric, categorical, or even custom objects.\n",
    "* Provides unique identifiers and facilitates data retrieval and selection.\n",
    "\n",
    "\n",
    "**D. Data Type:**\n",
    "\n",
    "* Defines the type of data stored in a Series or DataFrame column (e.g., integer, string, datetime).\n",
    "* Determines how data is stored and manipulated internally, impacting performance and operations.\n",
    "\n",
    "**E. BlockManager:**\n",
    "\n",
    "* (Internal) The core of Pandas, responsible for managing the physical memory layout of data in Series and DataFrames.\n",
    "* Uses NumPy arrays internally for efficient storage and retrieval of data.\n",
    "* We don't directly interact with this object, but it's crucial for Pandas functionality.\n",
    "\n",
    "\n",
    "**F. Panel (deprecated):**\n",
    "\n",
    "* A three-dimensional analogous to DataFrames, but less commonly used and deprecated in recent versions.\n",
    "* Considered less intuitive and efficient for most data analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Arguments for methods and classes throughout Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sep` character that is treated as delimiter in the file.\n",
    "- `delimiter` alias for `sep`\n",
    "- `header` row number contain column labels marking start of the data. By default it is None or 0 which means no header.\n",
    "- `names` sequence of column labels to apply. Basically if we want to give names to column we use it and if columns have already some name then set them to None as `header = 0` or `header = None` and then use `names`.\n",
    "- `index_col` columns to use as row labels, `index_col = False` can be used to force pandas to not use first column as row labels.\n",
    "- `usecols` subset of columns to select, denoted either by column labels or column indices. It specifies which columns header to use and for which not to. It can be an sequence or callable and where it evaluates True it will use header else not. Ex - `usecols = [True, False]` or `lambda x: x.upper() in ['aa', 'AA']`.\n",
    "- `dtype` if we want all columns to have single data type pass an string else if we want each columns to have different data types then pass an dictionary. Ex - `dtype='str` or `dtype='int'` or `dtype={'col1': 'int', 'col2': 'str'}`.\n",
    "- `engine` parser engine to use for data within dataframe or series. Argument can have following values `c`, `python`, `pyarrow` where `pyarrow` only supports multithreading.\n",
    "- `converters` functions for converting values in specified columns, it is an dict of {hashable: callable} where key can either be column or labels or column indices.\n",
    "- `true_values` values to consider as `True` n addition to case-insensitive variants of ‘True’. It is an `list`.\n",
    "- `false_values` values to consider as `False` n addition to case-insensitive variants of False. It is an `list`.\n",
    "- `skipinitialspace` its boolean which if `True` then skip spaces after delimiter else not.\n",
    "- `skiprows` its `int`, `list` of int or callable. Basically it specifies number of lines to skip at the start of file. Ex - To skip lines in all columns `skiprows=2`, to skip specified lines in each columns `skiprows=[2, 5, 8]`, to skip lines if condition is satisfied or via callable `lambda x: x in [0, 2]`.\n",
    "- `skipfooterint` default 0, its an `int`, its number of lines at bottom of file to skip (Unsupported with engine='c').\n",
    "- `nrows` its `int` and specifies number of lines to read from the file.\n",
    "- `na_values` it can be a hashable, iterable of hashable or dict of {hashable: iterable}. it specifies which type of Na values are in columns. Ex - for all the columns `na_values='NA'`, different type of na values in different columns `na_values=['NA', 'Nan']` or `na_values={'a': 'NA', 'b': 'Nan'}` or it may be callable also. All possible na values are: “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null“.\n",
    "- `na_filter` bool, default `True`, detect missing value markers (empty strings and the value of na_values). In data without any NA values, passing` na_filter=False` can improve the performance of reading a large file.\n",
    "- `verbosebool`, default `False`, Indicate number of NA values placed in non-numeric columns.\n",
    "- `skip_blank_lines` bool, default `True`, If `True`, skip over blank lines rather than interpreting as NaN values.\n",
    "- `parse_dates` bool, list of hashable, list of lists or dict of {hashable: list}, default is `False`.\n",
    "  - if `parse_dates` is `True` try parsing the index.\n",
    "  - if `parse_dates` is `list` of `int` or `str` try parsing columns each as a separate date column. Ex - `[1, 2, 3]` or `['a', 'b']`.\n",
    "  - if `parse_dates` is `list` of `list` combine columns and parse as a single date column and values are joined with space before parsing. Ex - `[[1, 2]]` it will combine values from column 1 and 2.\n",
    "  - if `parse_dates` is `dict` parse columns as date and call result the key, values are joined with space before parsing. Ex - `{'a': [1, 2]}` it will combine column 1 and 2 and call it `a`.\n",
    "- `keep_date_col` bool, default `False`, if `True` and `parse_dates` specifies combining multiple columns then keep the original columns else not.\n",
    "- `date_format` str or dict of column, format to use for parsing `parse_dates`. Ex - `%d%m%Y`.\n",
    "- `dayfirst` bool, default `False`, DD/MM formate dates, international and european format.\n",
    "- `cache_dates` bool, default `True`, if `True` use a cache of unique converted dates to apply the `datetime` conversion. It speeds up the parsing of duplicate date strings.\n",
    "- `iterator` bool, default `False`, return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.\n",
    "- `chunksize` int, optional, number of lines to read from the file per chunk.\n",
    "- `compression` str or dict, default `infer`,  or on-the-fly decompression of on-disk data, if `infer` it detects automatically some commonly used compression formats.\n",
    "- `thousands` str (length 1), optional, Character acting as the thousands separator in numerical values.\n",
    "- `decimal` str (length 1), default ‘.’, Character to recognize as decimal point (e.g., use ‘,’ for European data).\n",
    "- `lineterminator` str (length 1), optional, Character used to denote a line break. Only valid with C parser.\n",
    "- `quotechar` str (length 1), optional, Character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.\n",
    "- `quoting` {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL, \n",
    "Control field quoting behavior per csv.QUOTE_* constants. Default is csv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special characters are quoted (e.g., characters defined in quotechar, delimiter, or lineterminator.\n",
    "- `doublequote` bool, default True, When quotechar is specified and quoting is not `QUOTE_NONE`, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.\n",
    "- `escapechar` str (length 1), optional, Character used to escape other characters.\n",
    "- `comment` str (length 1), optional, Character indicating that the remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines.\n",
    "- `encoding` str, optional, default ‘utf-8’, Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python standard encodings .\n",
    "- `encoding_errors` str, optional, default ‘strict’, How encoding errors are treated.\n",
    "- `dialect` str or csv.Dialect, optional, it will override values (default or not) for the following parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`, and `quoting`. \n",
    "- `on_bad_lines` {‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’, Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :\n",
    "  - `error`, raise an Exception when a bad line is encountered.\n",
    "  - `warn`, raise a warning when a bad line is encountered and skip that line.\n",
    "  - `skip`, skip bad lines without raising or warning when they are encountered.\n",
    "- `delim_whitespace` bool, default `False`, specifies whether white space will be used as `sep`.\n",
    "- `low_memory` bool, default True, Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter.\n",
    "- `memory_map` bool, default False, if a filepath is provided for `filepath_or_buffer`, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.\n",
    "- `float_precision` {‘high’, ‘legacy’, ‘round_trip’}, optional, specifies which converter the C engine should use for floating-point values. The options are None or `high` for the ordinary converter, `legacy` for the original lower precision pandas converter, and `round_trip` for the round-trip converter.\n",
    "- `storage_options` dict, optional, Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. \n",
    "- `dtype_backend `{‘numpy_nullable’, ‘pyarrow’}, default, ‘numpy_nullable’ Back-end data type applied to the resultant DataFrame (still experimental). Behavior is as follows:\n",
    "  - `numpy_nullable`: returns nullable-dtype-backed DataFrame (default).\n",
    "  - `pyarrow`: returns pyarrow-backed nullable ArrowDtype DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input/Output Functions in Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various input/output general functions are available in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_pickle(filepath_or_buffer, compression='infer', storage_options=None)` Load pickled pandas object (or any object) from file.\n",
    "  - `compression` For on-the-fly decompression of on-disk data, if `infer` it detects automatically some commonly used compression formats.\n",
    "  - `storage_options` used to store pickled object to specified connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Flat File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_table(filepath_or_buffer, *[, sep, ...])` Read general delimited file into DataFrame.\n",
    "- `read_csv(filepath_or_buffer, *[, sep, ...])` Read a comma-separated values (csv) file into DataFrame.\n",
    "- `read_fwf(filepath_or_buffer, *[, colspecs, ...])`  Read a table of fixed-width formatted lines into DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Clipboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_clipboard([sep, dtype_backend])` Read text from clipboard and pass to `read_csv()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_excel(io[, sheet_name, header, names, ...])` Read an Excel file into a pandas DataFrame.\n",
    "- `ExcelFile(path_or_buffer[, engine, ...])` Class for parsing tabular Excel sheets into DataFrame objects.\n",
    "- `ExcelFile.parse([sheet_name, header, names, ...])` Parse specified sheet(s) into a DataFrame.\n",
    "- `Styler.to_excel(excel_writer[, sheet_name, ...])`  Write Styler to an Excel sheet, where `excel_writer` is path of existing excel writer.\n",
    "- `ExcelWriter(path[, engine, date_format, ...])` Class for writing DataFrame objects into excel sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_json(path_or_buf, *[, orient, typ, ...])` Convert a JSON string to pandas object.\n",
    "- `json_normalize(data[, record_path, meta, ...])` Normalize semi-structured JSON data into a flat table.\n",
    "- `build_table_schema(data[, index, ...])` Create a Table schema from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_html(io, *[, match, flavor, header, ...])` Read HTML tables into a list of DataFrame objects.\n",
    "- `Styler.to_html([buf, table_uuid, ...])` Write Styler to a file, buffer or string in HTML-CSS format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `read_xml(path_or_buffer, *[, xpath, ...])` Read XML document into a DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Styler.to_latex([buf, column_format, ...])` Write Styler to a file, buffer or string in LaTeX format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
